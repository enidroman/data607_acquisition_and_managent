---
title: "Data 607 Sentiment Analysis With Tidy Data, Part 2"
author: "Enid Roman"
date: "2022-11-04"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### **PART 2 OF 2 SENTIMENT ANALYSIS WITH TIDY DATASET**

#### My corpus is from the Harry Potter series title “Half-Blood Prince” by J. K. Rowling. 

#### Package was created by Bradley Boehmke and I retrieved it from https://afit-r.github.io/sentiment_analysis.

```{r}
# Install Libraries I felt I Needed

#install.packages("tidyverse")
#install.packages("textdata")
#install.packages("gutenbergr")
#install.packages("DT")
#install.packages("flextable")
#install.packages("wordcloud")
#devtools::install_github("ropensci/gutenbergr")
if (packageVersion("devtools") < 1.6) {
  install.packages("devtools")
}
devtools::install_github("bradleyboehmke/harrypotter")
```

### **THE SENTIMENTS DATASETS**

```{r}
# Load Libraries

library(tidyverse)      # data manipulation & plotting
library(stringr)        # text cleaning and regular expressions
library(tidytext)       # provides additional text mining functions
library(textdata)       # Provides a framework to download, parse, and store text datasets on the disk and load                           them when needed. Includes various sentiment lexicons and labeled text data sets for                           classification and analysis.
library(dplyr)          # aims to provide a function for each basic verb of data manipulation.
library(harrypotter)    # provides the first seven novels of the Harry Potter series
library(ggplot2)        # is an open-source data visualization package for the statistical programming language                           R
```

### **HARRY POTTER NOVELS IN THIS LIBRARY**

#### philosophers_stone: Harry Potter and the Philosophers Stone (1997)
#### chamber_of_secrets: Harry Potter and the Chamber of Secrets (1998)
#### prisoner_of_azkaban: Harry Potter and the Prisoner of Azkaban (1999)
#### goblet_of_fire: Harry Potter and the Goblet of Fire (2000)
#### order_of_the_phoenix: Harry Potter and the Order of the Phoenix (2003)
#### half_blood_prince: Harry Potter and the Half-Blood Prince (2005)
#### deathly_hallows: Harry Potter and the Deathly Hallows (2007)

#### To perform sentiment analysis we need to have our data in a tidy format. The following converts all seven Harry Potter novels into a tibble that has each word by chapter by book. 

```{r}
titles <- c("Philosopher's Stone", "Chamber of Secrets", "Prisoner of Azkaban",
            "Goblet of Fire", "Order of the Phoenix", "Half-Blood Prince",
            "Deathly Hallows")

books <- list(philosophers_stone, chamber_of_secrets, prisoner_of_azkaban,
           goblet_of_fire, order_of_the_phoenix, half_blood_prince,
           deathly_hallows)
  
series <- tibble()

for(i in seq_along(titles)) {
        
        clean <- tibble(chapter = seq_along(books[[i]]),
                        text = books[[i]]) %>%
             unnest_tokens(word, text) %>%
             mutate(book = titles[i]) %>%
             select(book, everything())

        series <- rbind(series, clean)
}

# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))

series
```
### **SENTIMENT ANALYSIS WITH INNER JOIN**

#### We use the nrc sentiment data set to assess the different sentiments that are represented across the Harry Potter series. There is a stronger negative presence than positive.

```{r}
series %>%
        right_join(get_sentiments("nrc")) %>%
        filter(!is.na(sentiment)) %>%
        count(sentiment, sort = TRUE)
```
#### To visualize this analysis, we plot these sentiment scores across the plot trajectory of each novel. We are plotting against the index on the x-axis that keeps track of narrative time in sections of text. 

#### We perform the following:

#### 1. Create an index that breaks up each book by 500 words; this is the approximate number of words on every two pages so this will allow us to assess changes in sentiment even within chapters.
#### 2. Join the bing lexicon with inner_join to assess the positive vs. negative sentiment of each word.
#### 3. Count up how many positive and negative words there are for every two pages”.
#### 4. Spread our data and…
#### 5. Calculate a net sentiment (positive - negative).
#### 6. Plot our data. 

#### We can see how the plot of each novel changes toward more positive or negative sentiment over the trajectory of the story.

```{r}
series %>%
        group_by(book) %>% 
        mutate(word_count = 1:n(),
               index = word_count %/% 500 + 1) %>% 
        inner_join(get_sentiments("bing")) %>%
        count(book, index = index , sentiment) %>%
        ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative,
               book = factor(book, levels = titles)) %>%
        ggplot(aes(index, sentiment, fill = book)) +
          geom_bar(alpha = 0.5, stat = "identity", show.legend = FALSE) +
          facet_wrap(~ book, ncol = 2, scales = "free_x")
```
### **COMPARING THE THREE SENTIMENTS**

#### We compare each sentiments to get more information on which one is appropriate for your purposes. Lets use all three sentiment lexicons and examine how they differ for each novel.

```{r}
afinn <- series %>%
        group_by(book) %>% 
        mutate(word_count = 1:n(),
               index = word_count %/% 500 + 1) %>% 
        inner_join(get_sentiments("afinn")) %>%
        group_by(book, index) %>%
        summarise(sentiment = sum(value)) %>%
        mutate(method = "AFINN")

bing_and_nrc <- bind_rows(series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>% 
                  inner_join(get_sentiments("bing")) %>%
                  mutate(method = "Bing"),
          series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>%
                  inner_join(get_sentiments("nrc") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "NRC")) %>%
        count(book, method, index = index , sentiment) %>%
        ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative) %>%
        select(book, index, method, sentiment)
```

#### We now have an estimate of the net sentiment (positive - negative) in each chunk of the novel text for each sentiment lexicon. We bind them together and plot them.

#### The three different lexicons for calculating sentiment give results that are different in an absolute sense but have fairly similar relative trajectories through the novels. We see similar dips and peaks in sentiment at about the same places in the novel, but the absolute values are significantly different. In some instances, it apears the AFINN lexicon finds more positive sentiments than the Bing and NRC lexicon. This output also allows us to compare across novels. First, you get a good sense of differences in book lengths - Order of the Pheonix is much longer than Philosopher’s Stone. Second, you can compare how books differ in their sentiment (both direction and magnitude) across a series.

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
        ungroup() %>%
        mutate(book = factor(book, levels = titles)) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
  facet_grid(book ~ method)
```
### **COMMON POSITIVE AND NEGATIVE SENTIMENT WORDS**

#### We can analyze word counts that contribute to each sentiment.

```{r}
bing_word_counts <- series %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```
#### By doing a geomplot we can view this visually to assess the top n words for each sentiment.

```{r}
bing_word_counts %>%
        group_by(sentiment) %>%
        top_n(10) %>%
        ggplot(aes(reorder(word, n), n, fill = sentiment)) +
          geom_bar(alpha = 0.8, stat = "identity", show.legend = FALSE) +
          facet_wrap(~sentiment, scales = "free_y") +
          labs(y = "Contribution to sentiment", x = NULL) +
          coord_flip()
```
#### We see an anomaly in the sentiment analysis; the word “fudge” is coded as negative but it is used as a type of food in Harry Potter series. If it were appropriate for our purposes, we could easily add “fudge” to a custom stop-words list using bind_rows(). We could implement that with a strategy such as this.

```{r}
custom_stop_words <- bind_rows(tibble(word = c("fudge"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```
#### Using the wordcloud package, which uses base R graphics. Let’s look at the most common words in the Harry Potter Series.

#### The size of a word’s text below is in proportion to its frequency within its sentiment. We can use this visualization to see the most important positive and negative words, but the sizes of the words are not comparable across sentiments.

```{r}
library(wordcloud)
library(reshape2)

series %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```
### **FURTER ANALYSIS**

#### I chose to analyze one of my favorite Harry Pottr novels, Half-Blood Prince. 

#### Here I demonstrate a sample of the Half-Blood Prince. The following illustrates the raw text of the first chapter of the half_blood_prince. Each text is in a character vector with each element representing a single chapter. 

```{r}
half_blood_prince[1:1]
```

### **THE SENTIMENTS DATASETS**

#### Import the novel half_blood_prince.

#### Code based on https://afit-r.github.io/sentiment_analysis.

```{r}
titles <- c("Half-Blood Prince")
books <- list(half_blood_prince)
series <- tibble()

for(i in seq_along(titles)) {
  
  temp <- tibble(chapter = seq_along(books[[i]]),
                  text = books[[i]]) %>%
    unnest_tokens(word, text) %>%
    ##Here we tokenize each chapter into words
    mutate(book = titles[i]) %>%
    select(book, everything())
  
  series <- rbind(series, temp)
}
# set factor to keep books in order of publication
series$book <- factor(series$book, levels = rev(titles))

# This is what the tokenizing looks like

series
```
#### Here we see the number of chapters Half-Blood Prince contain and a glimpse of the first sentence in the first chapter.

```{r}

str(half_blood_prince)
```

### **SENTIMENT ANALYSIS WITH INNER JOIN**

#### We compare each sentiments to get more information on which one is appropriate for your purposes. Lets use all three sentiment lexicons and examine how they differ in this particular novel, Half-Blood Prince.

```{r}
# Using the AFINN lexicon for sentiment analysis on Harry Potter

afinn <- series %>%
        group_by(book) %>% 
        mutate(word_count = 1:n(),
               index = word_count %/% 500 + 1) %>% 
        inner_join(get_sentiments("afinn")) %>%
        group_by(book, index) %>%
        summarise(sentiment = sum(value)) %>%
        mutate(method = "AFINN")

bing_and_nrc <- bind_rows(series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>% 
                  inner_join(get_sentiments("bing")) %>%
                  mutate(method = "Bing"),
          series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>%
                  inner_join(get_sentiments("nrc") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "NRC")) %>%
        count(book, method, index = index , sentiment) %>%
        ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative) %>%
        select(book, index, method, sentiment)
```

#### We now have an estimate of the net sentiment (positive - negative) in each chunk of the novel text for each sentiment lexicon. We bind them together and plot them.

#### The three different lexicons for calculating sentiment give results that are different in an absolute sense but have fairly similar relative trajectories through the novel. We see similar dips and peaks in sentiment at about the same places in the novel, but the absolute values are significantly different. In some instances, it apears the AFINN lexicon finds more positive sentiments than the Bing and NRC lexicon. NRC seems more negative sentiments then the other two sentiments. This output also allows us to compare across the chapters of the novel. You can compare how the chapters differ in their sentiment (both direction and magnitude) across the series.

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
### Why the result for the NRC lexicon biased so high in sentiment compared to the Bing et al. result? Here we see how many positive and negative words are in these lexicons.

### Both lexicons have more negative than positive words, but the ratio of negative to positive words is higher in the Bing lexicon than the NRC lexicon. This will contribute to the effect we see in the plot above, as will any systematic difference in word matches. Whatever the source of these differences, we see similar relative trajectories across the narrative arc, with similar changes in slope, but marked differences in absolute sentiment from lexicon to lexicon. This is all important context to keep in mind when choosing a sentiment lexicon for analysis.

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```
### **MOST COMMON POSITIVE AND NEGATIVE WORDS**

#### We can analyze word counts that contribute to each sentiment. By implementing count() here with arguments of both word and sentiment, we find out how much each word contributed to each sentiment.

```{r}
bing_word_counts <- series %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
```
#### #### By doing a geomplot we can view this visually to assess the top n words for each sentiment.

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

#### We see an anomaly in the sentiment analysis; the word “fudge” is coded as negative but it is used as a type of food in Harry Potter series. If it were appropriate for our purposes, we could easily add “fudge” to a custom stop-words list using bind_rows(). We could implement that with a strategy such as this.

```{r}
custom_stop_words <- bind_rows(tibble(word = c("fudge"),  
                                      lexicon = c("custom")), 
                               stop_words)

custom_stop_words
```
#### Using the wordcloud package, which uses base R graphics. Let’s look at the most common words in the Harry Potter Series.

#### The size of a word’s text below is in proportion to its frequency within its sentiment. We can use this visualization to see the most important positive and negative words, but the sizes of the words are not comparable across sentiments.

```{r}
library(wordcloud)
library(reshape2)

series %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```
### **THE LOUGHRAN LEXICON FOR SENTIMENT ANALYSIS**

#### # Using the Loughran lexicon for sentiment analysis on Harry Potter.

#### The loughran lexicon divided words into constraining, litigious, negative, positive, superfluous and uncertainty. 

```{r}
# Using the Loughran lexicon for sentiment analysis on Harry Potter
loughran <- series %>%
  right_join(get_sentiments("loughran")) %>%
  filter(!is.na(sentiment)) %>%
  count(sentiment, sort = TRUE)
```

#### We can see that in loughran negative words are more common than positive words while in bind this proportion is a bit over 4.

```{r}
#A view of the Loughran analysis

loughran

```
#### We compare each sentiments to get more information on which one is appropriate for your purposes. Lets use all four sentiment lexicons and examine how they differ in this particular novel, Half-Blood Prince.

```{r}
#Prepares loughran for plotting
afinn <- series %>%
        group_by(book) %>% 
        mutate(word_count = 1:n(),
               index = word_count %/% 500 + 1) %>% 
        inner_join(get_sentiments("afinn")) %>%
        group_by(book, index) %>%
        summarise(sentiment = sum(value)) %>%
        mutate(method = "AFINN")

bing_and_nrc <- bind_rows(series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>% 
                  inner_join(get_sentiments("bing")) %>%
                  mutate(method = "Bing"),
          series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>%
                  inner_join(get_sentiments("nrc") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "NRC"))

loughran <- bind_rows(series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>%
                  inner_join(get_sentiments("loughran") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "Loughran")) %>%
        count(book, method, index = index , sentiment) %>%
        ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative) %>%
        select(book, index, method, sentiment)
```


```{r}

# In order for the plot to work I need to bring this down from row 270.

afinn <- series %>%
        group_by(book) %>% 
        mutate(word_count = 1:n(),
               index = word_count %/% 500 + 1) %>% 
        inner_join(get_sentiments("afinn")) %>%
        group_by(book, index) %>%
        summarise(sentiment = sum(value)) %>%
        mutate(method = "AFINN")

bing_and_nrc <- bind_rows(series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>% 
                  inner_join(get_sentiments("bing")) %>%
                  mutate(method = "Bing"),
          series %>%
                  group_by(book) %>% 
                  mutate(word_count = 1:n(),
                         index = word_count %/% 500 + 1) %>%
                  inner_join(get_sentiments("nrc") %>%
                                     filter(sentiment %in% c("positive", "negative"))) %>%
                  mutate(method = "NRC")) %>%
        count(book, method, index = index , sentiment) %>%
        ungroup() %>%
        spread(sentiment, n, fill = 0) %>%
        mutate(sentiment = positive - negative) %>%
        select(book, index, method, sentiment)
```


#### We now have an estimate of the net sentiment (positive - negative) in each chunk of the novel text for each sentiment lexicon. We bind them together and plot them.

#### The four different lexicons for calculating sentiment give results that are different in an absolute sense but have fairly similar relative trajectories through the novel. We see similar dips and peaks in sentiment at about the same places in the novel, but the absolute values are significantly different. In some instances, it apears the AFINN lexicon finds more positive sentiments than the Bing, NRC and Loughran lexicon. NRC and Loughran seems more negative sentiments then the other two sentiments. This output also allows us to compare across the chapters of the novel. You can compare how the chapters differ in their sentiment (both direction and magnitude) across the series.

```{r}
# Please note need to rerun line 270 alone in order for this plot to run. 

bind_rows(afinn, 
          bing_and_nrc, loughran) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```
#### The Stanford CoreNLP tools and the sentimentr R package (currently available on Github but not CRAN) are examples of such sentiment analysis algorithms. For these, we may want to tokenize text into sentences. 

```{r}
tibble(text = half_blood_prince) %>% 
  unnest_tokens(sentence, text, token = "sentences")
```
#### **CONCLUSION**

#### Sentiment analysis provides a way to understand the attitudes and opinions expressed in texts. We explored how to approach sentiment analysis using tidy data principles; when text data is in a tidy data structure, sentiment analysis can be implemented as an inner join. We can use sentiment analysis to understand how a narrative arc changes throughout its course or what words with emotional and opinion content are important for a particular text.  
### We can clearly see that the lexicon that is chosen can have a big impact on the analysis and we need to be careful to take this into consideration on any analysis.